import os
import discord
from discord.ext import commands
from dotenv import load_dotenv
import asyncio
import time
import wave
import io
import json
import tempfile
import threading
import queue
from collections import deque
from typing import Optional, Dict, Any, List
import numpy as np

# faster-whisperã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from faster_whisper import WhisperModel

# discord-ext-voice-recv ã®æ­£ã—ã„ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–¹æ³•
from discord.ext.voice_recv import VoiceRecvClient
from discord.ext.voice_recv import AudioSink
from discord.ext.voice_recv import VoiceData

# VAD (Voice Activity Detection) ç”¨
import webrtcvad

# â˜…â˜…â˜… æ–°ã—ã„è¨­å®š â˜…â˜…â˜…
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§å‘ä¸Šã®ãŸã‚ã®è¨­å®š
REALTIME_CHUNK_DURATION_MS = 500  # 500msï¼ˆ0.5ç§’ï¼‰ã§ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
VAD_AGGRESSIVENESS = 2  # VADã®æ„Ÿåº¦ (0-3, 3ãŒæœ€ã‚‚å³æ ¼)
MIN_SPEECH_DURATION_MS = 300  # æœ€å°ç™ºè©±æ™‚é–“ï¼ˆ300msï¼‰
SILENCE_THRESHOLD_MS = 800  # ç„¡éŸ³æ™‚é–“ãŒã“ã‚Œã‚’è¶…ãˆã‚‹ã¨ç™ºè©±çµ‚äº†ã¨ã¿ãªã™
OVERLAP_DURATION_MS = 200  # ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—

# éŸ³å£°å“è³ªè¨­å®šï¼ˆ16kHzã«å¤‰æ›´ã—ã¦Whisperã®å‡¦ç†ã‚’é«˜é€ŸåŒ–ï¼‰
WHISPER_SAMPLE_RATE = 16000  # Whisperã®æ¨å¥¨ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
PCM_SAMPLE_RATE = 48000  # Discordã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿
PCM_BYTES_PER_SAMPLE = 2
PCM_CHANNELS = 2

# è¨ˆç®—ç”¨å®šæ•°
REALTIME_CHUNK_SAMPLES = int(WHISPER_SAMPLE_RATE * REALTIME_CHUNK_DURATION_MS / 1000)
REALTIME_CHUNK_BYTES = REALTIME_CHUNK_SAMPLES * 2  # 16-bit mono

# â˜…â˜…â˜… å¾“æ¥ã®è¨­å®šï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰ â˜…â˜…â˜…
# TRANSCRIPTION_CHUNK_DURATION_SECONDS = 2
# TRANSCRIPTION_CHUNK_BYTES = PCM_SAMPLE_RATE * PCM_BYTES_PER_SAMPLE * PCM_CHANNELS * TRANSCRIPTION_CHUNK_DURATION_SECONDS

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

DISCORD_BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN")

# Botã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚’è¨­å®š
intents = discord.Intents.default()
intents.message_content = True
intents.voice_states = True
intents.members = True

# Botã®åˆæœŸåŒ–
bot = commands.Bot(command_prefix='!', intents=intents)

# éŸ³å£°ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
AUDIO_OUTPUT_DIR = "recorded_audio"
os.makedirs(AUDIO_OUTPUT_DIR, exist_ok=True)

# ã‚®ãƒ«ãƒ‰ã”ã¨ã®ãƒœã‚¤ã‚¹æ¥ç¶šã‚’ç®¡ç†
connections: Dict[int, VoiceRecvClient] = {}

# Whisperãƒ¢ãƒ‡ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
WHISPER_MODEL: Optional[WhisperModel] = None
WHISPER_MODEL_SIZE = "base"
WHISPER_DEVICE = "cpu"
WHISPER_COMPUTE_TYPE = "int8"


class AudioUtils:
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def resample_audio(audio_data: bytes, original_rate: int, target_rate: int) -> bytes:
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if original_rate == target_rate:
            return audio_data
        
        # NumPyã‚’ä½¿ç”¨ã—ã¦ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        audio_array = np.frombuffer(audio_data, dtype=np.int16)
        
        # ã‚¹ãƒ†ãƒ¬ã‚ªã‹ã‚‰ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
        if len(audio_array) % 2 == 0:
            audio_array = audio_array.reshape(-1, 2).mean(axis=1).astype(np.int16)
        
        # ç°¡æ˜“ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚ˆã‚Šé«˜å“è³ªãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ï¼‰
        ratio = target_rate / original_rate
        new_length = int(len(audio_array) * ratio)
        resampled = np.interp(
            np.linspace(0, len(audio_array) - 1, new_length),
            np.arange(len(audio_array)),
            audio_array
        ).astype(np.int16)
        
        return resampled.tobytes()
    
    @staticmethod
    def apply_vad(audio_data: bytes, sample_rate: int, vad_aggressiveness: int = 2) -> bool:
        """WebRTC VADã‚’ä½¿ç”¨ã—ã¦éŸ³å£°æ´»å‹•ã‚’æ¤œå‡º"""
        try:
            vad = webrtcvad.Vad(vad_aggressiveness)
            
            # VADã¯ç‰¹å®šã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºã«å¯¾å¿œ
            if sample_rate not in [8000, 16000, 32000, 48000]:
                return True  # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã®å ´åˆã¯éŸ³å£°ã‚ã‚Šã¨ã¿ãªã™
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºï¼ˆ10ms, 20ms, 30msï¼‰
            frame_duration_ms = 30
            frame_size = int(sample_rate * frame_duration_ms / 1000) * 2  # 16-bit
            
            if len(audio_data) < frame_size:
                return False
                
            # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åˆ‡ã‚Šå‡ºã—ã¦VADã‚’é©ç”¨
            for i in range(0, len(audio_data) - frame_size + 1, frame_size):
                frame = audio_data[i:i + frame_size]
                if vad.is_speech(frame, sample_rate):
                    return True
            
            return False
        except Exception as e:
            print(f"VADã‚¨ãƒ©ãƒ¼: {e}")
            return True  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯éŸ³å£°ã‚ã‚Šã¨ã¿ãªã™


class RealtimeTranscriptionEngine:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, whisper_model: WhisperModel, output_dir: str):
        self.whisper_model = whisper_model
        self.output_dir = output_dir
        self.transcription_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.is_running = False
        self.worker_thread = None
        
    def start(self):
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚’é–‹å§‹"""
        if not self.is_running:
            self.is_running = True
            self.worker_thread = threading.Thread(target=self._transcription_worker, daemon=True)
            self.worker_thread.start()
            print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
    
    def stop(self):
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚’åœæ­¢"""
        self.is_running = False
        if self.worker_thread:
            self.worker_thread.join(timeout=5.0)
            print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åœæ­¢ã—ã¾ã—ãŸ")
    
    def submit_audio(self, audio_data: bytes, user_id: int, username: str, guild_id: int):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
        if self.is_running:
            self.transcription_queue.put({
                'audio_data': audio_data,
                'user_id': user_id,
                'username': username,
                'guild_id': guild_id,
                'timestamp': time.time()
            })
    
    def get_result(self) -> Optional[Dict]:
        """å‡¦ç†çµæœã‚’å–å¾—"""
        try:
            return self.result_queue.get_nowait()
        except queue.Empty:
            return None
    
    def _transcription_worker(self):
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§æ–‡å­—èµ·ã“ã—ã‚’å‡¦ç†ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        while self.is_running:
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ï¼ˆ0.1ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
                task = self.transcription_queue.get(timeout=0.1)
                
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’WAVãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›
                audio_data = task['audio_data']
                if len(audio_data) < REALTIME_CHUNK_BYTES:
                    continue
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:
                    temp_path = temp_file.name
                
                try:
                    # WAVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                    with wave.open(temp_path, 'wb') as wf:
                        wf.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«
                        wf.setsampwidth(2)  # 16-bit
                        wf.setframerate(WHISPER_SAMPLE_RATE)
                        wf.writeframes(audio_data)
                    
                    # Whisperã§æ–‡å­—èµ·ã“ã—
                    if self.whisper_model:
                        segments, info = self.whisper_model.transcribe(
                            temp_path,
                            language="ja",
                            beam_size=1,  # é«˜é€ŸåŒ–ã®ãŸã‚ãƒ“ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’1ã«
                            vad_filter=True,
                            no_speech_threshold=0.6,
                            condition_on_previous_text=False  # å‰ã®ãƒ†ã‚­ã‚¹ãƒˆã«ä¾å­˜ã—ãªã„
                        )
                        
                        transcription = ""
                        for segment in segments:
                            transcription += segment.text
                        
                        if transcription.strip():
                            self.result_queue.put({
                                'user_id': task['user_id'],
                                'username': task['username'],
                                'guild_id': task['guild_id'],
                                'transcription': transcription.strip(),
                                'timestamp': task['timestamp']
                            })
                
                finally:
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    try:
                        os.remove(temp_path)
                    except:
                        pass
            
            except queue.Empty:
                continue
            except Exception as e:
                print(f"æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")


class StreamingAudioBuffer:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éŸ³å£°ãƒãƒƒãƒ•ã‚¡"""
    
    def __init__(self, user_id: int, username: str):
        self.user_id = user_id
        self.username = username
        self.audio_buffer = deque()
        self.last_speech_time = 0
        self.is_speaking = False
        self.accumulated_audio = bytearray()
        
    def add_audio(self, audio_data: bytes):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        current_time = time.time()
        
        # 16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        resampled_audio = AudioUtils.resample_audio(
            audio_data, PCM_SAMPLE_RATE, WHISPER_SAMPLE_RATE
        )
        
        # VADã§éŸ³å£°æ´»å‹•ã‚’æ¤œå‡º
        has_speech = AudioUtils.apply_vad(resampled_audio, WHISPER_SAMPLE_RATE, VAD_AGGRESSIVENESS)
        
        if has_speech:
            self.last_speech_time = current_time
            self.is_speaking = True
            self.accumulated_audio.extend(resampled_audio)
        else:
            # ç„¡éŸ³æ™‚é–“ãŒé–¾å€¤ã‚’è¶…ãˆãŸå ´åˆ
            if self.is_speaking and (current_time - self.last_speech_time) > (SILENCE_THRESHOLD_MS / 1000):
                self.is_speaking = False
                return True  # ç™ºè©±çµ‚äº†ã‚’ç¤ºã™
            elif self.is_speaking:
                # ã¾ã ç™ºè©±ä¸­ã®å ´åˆã¯ç„¡éŸ³éƒ¨åˆ†ã‚‚å«ã‚ã‚‹
                self.accumulated_audio.extend(resampled_audio)
        
        return False
    
    def get_audio_chunk(self) -> Optional[bytes]:
        """éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—"""
        if len(self.accumulated_audio) >= REALTIME_CHUNK_BYTES:
            chunk = bytes(self.accumulated_audio[:REALTIME_CHUNK_BYTES])
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®ãŸã‚ã«ä¸€éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’æ®‹ã™
            overlap_bytes = int(REALTIME_CHUNK_BYTES * OVERLAP_DURATION_MS / REALTIME_CHUNK_DURATION_MS)
            self.accumulated_audio = self.accumulated_audio[REALTIME_CHUNK_BYTES - overlap_bytes:]
            return chunk
        return None
    
    def get_remaining_audio(self) -> Optional[bytes]:
        """æ®‹ã‚Šã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if len(self.accumulated_audio) > 0:
            chunk = bytes(self.accumulated_audio)
            self.accumulated_audio.clear()
            return chunk
        return None


class RealtimeVoiceProcessor:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""
    
    def __init__(self, transcription_engine: RealtimeTranscriptionEngine):
        self.transcription_engine = transcription_engine
        self.audio_buffers: Dict[int, Dict[int, StreamingAudioBuffer]] = {}  # guild_id -> user_id -> buffer
        self.text_channels: Dict[int, discord.TextChannel] = {}  # guild_id -> channel
        self.result_polling_tasks: Dict[int, asyncio.Task] = {}  # guild_id -> task
        
    def start_processing(self, guild_id: int, text_channel: discord.TextChannel):
        """å‡¦ç†ã‚’é–‹å§‹"""
        self.text_channels[guild_id] = text_channel
        if guild_id not in self.audio_buffers:
            self.audio_buffers[guild_id] = {}
        
        # çµæœãƒãƒ¼ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        if guild_id not in self.result_polling_tasks or self.result_polling_tasks[guild_id].done():
            self.result_polling_tasks[guild_id] = asyncio.create_task(
                self._result_polling_loop(guild_id)
            )
        
        self.transcription_engine.start()
        print(f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°å‡¦ç†ã‚’é–‹å§‹: Guild {guild_id}")
    
    def stop_processing(self, guild_id: int):
        """å‡¦ç†ã‚’åœæ­¢"""
        if guild_id in self.result_polling_tasks:
            self.result_polling_tasks[guild_id].cancel()
            del self.result_polling_tasks[guild_id]
        
        if guild_id in self.audio_buffers:
            del self.audio_buffers[guild_id]
        
        if guild_id in self.text_channels:
            del self.text_channels[guild_id]
        
        print(f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°å‡¦ç†ã‚’åœæ­¢: Guild {guild_id}")
    
    def process_audio(self, guild_id: int, user_id: int, username: str, audio_data: bytes):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        if guild_id not in self.audio_buffers:
            return
        
        if user_id not in self.audio_buffers[guild_id]:
            self.audio_buffers[guild_id][user_id] = StreamingAudioBuffer(user_id, username)
        
        buffer = self.audio_buffers[guild_id][user_id]
        
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        speech_ended = buffer.add_audio(audio_data)
        
        # ãƒãƒ£ãƒ³ã‚¯ãŒæº–å‚™ã§ãã¦ã„ã‚‹ã‹ç¢ºèª
        chunk = buffer.get_audio_chunk()
        if chunk:
            self.transcription_engine.submit_audio(chunk, user_id, username, guild_id)
        
        # ç™ºè©±ãŒçµ‚äº†ã—ãŸå ´åˆã€æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        if speech_ended:
            remaining = buffer.get_remaining_audio()
            if remaining and len(remaining) >= REALTIME_CHUNK_BYTES // 2:  # åŠåˆ†ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                self.transcription_engine.submit_audio(remaining, user_id, username, guild_id)
    
    async def _result_polling_loop(self, guild_id: int):
        """çµæœãƒãƒ¼ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                result = self.transcription_engine.get_result()
                if result and result['guild_id'] == guild_id:
                    text_channel = self.text_channels.get(guild_id)
                    if text_channel:
                        await text_channel.send(f"**{result['username']}**: {result['transcription']}")
                        print(f"æ–‡å­—èµ·ã“ã—é€ä¿¡: {result['username']} -> {result['transcription']}")
                
                await asyncio.sleep(0.05)  # 50msã”ã¨ã«ãƒã‚§ãƒƒã‚¯
            except Exception as e:
                print(f"çµæœãƒãƒ¼ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                break


# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
transcription_engine: Optional[RealtimeTranscriptionEngine] = None
voice_processor: Optional[RealtimeVoiceProcessor] = None


class OptimizedAudioSink(AudioSink):
    """æœ€é©åŒ–ã•ã‚ŒãŸéŸ³å£°ã‚·ãƒ³ã‚¯"""
    
    def __init__(self, processor: RealtimeVoiceProcessor, guild_id: int):
        super().__init__()
        self.processor = processor
        self.guild_id = guild_id
        
    def write(self, user: discord.Member, data: VoiceData):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡"""
        if user.bot:
            return
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
        self.processor.process_audio(
            self.guild_id, 
            user.id, 
            user.display_name, 
            data.pcm
        )
    
    def wants_opus(self) -> bool:
        return False
    
    def cleanup(self):
        pass


# Botã‚³ãƒãƒ³ãƒ‰
@bot.command()
async def join(ctx):
    """ãƒœãƒƒãƒˆã‚’ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ«ã«æ¥ç¶š"""
    if ctx.author.voice is None:
        await ctx.send("âŒ ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ«ã«æ¥ç¶šã—ã¦ãã ã•ã„ã€‚")
        return

    voice_channel = ctx.author.voice.channel
    
    # æ—¢å­˜ã®æ¥ç¶šãŒã‚ã‚Œã°åˆ‡æ–­
    if ctx.guild.id in connections:
        await connections[ctx.guild.id].disconnect()
        voice_processor.stop_processing(ctx.guild.id)
        del connections[ctx.guild.id]
        await asyncio.sleep(0.5)

    # VoiceRecvClient ã‚’ä½¿ç”¨ã—ã¦æ¥ç¶š
    vc = await voice_channel.connect(cls=VoiceRecvClient, reconnect=True)
    connections[ctx.guild.id] = vc

    # éŸ³å£°ã‚·ãƒ³ã‚¯ã‚’è¨­å®š
    vc.listen(OptimizedAudioSink(voice_processor, ctx.guild.id))
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã‚’é–‹å§‹
    voice_processor.start_processing(ctx.guild.id, ctx.channel)

    await ctx.send(f'ğŸµ ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ« **{voice_channel.name}** ã«æ¥ç¶šã—ã¾ã—ãŸï¼')
    await ctx.send(
        f"ğŸ™ï¸ **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼**\n"
        f"âš¡ å‡¦ç†é–“éš”: {REALTIME_CHUNK_DURATION_MS}ms\n"
        f"ğŸ¯ VADæ„Ÿåº¦: {VAD_AGGRESSIVENESS}/3\n"
        f"ğŸ”Š éŸ³å£°æ¤œå‡ºé–¾å€¤: {SILENCE_THRESHOLD_MS}ms\n"
        f"â„¹ï¸ `!leave` ã§æ¥ç¶šã‚’åˆ‡æ–­ã§ãã¾ã™ã€‚"
    )


@bot.command()
async def leave(ctx):
    """ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰åˆ‡æ–­"""
    if ctx.guild.id not in connections:
        await ctx.send("âŒ ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ«ã«æ¥ç¶šã—ã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    voice_processor.stop_processing(ctx.guild.id)
    await connections[ctx.guild.id].disconnect()
    del connections[ctx.guild.id]
    await ctx.send("ğŸ‘‹ ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰åˆ‡æ–­ã—ã¾ã—ãŸã€‚")


@bot.command()
async def status(ctx):
    """ç¾åœ¨ã®çŠ¶æ³ã‚’ç¢ºèª"""
    if ctx.guild.id not in connections:
        await ctx.send("âŒ ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ«ã«æ¥ç¶šã—ã¦ã„ã¾ã›ã‚“ã€‚")
        return
    
    vc = connections[ctx.guild.id]
    channel_members = len(vc.channel.members) - 1
    
    status_msg = f"ğŸ“Š **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ç¨¼åƒä¸­**\n"
    status_msg += f"ğŸ¤ ãƒãƒ£ãƒ³ãƒãƒ«å‚åŠ è€…: {channel_members}äºº\n"
    status_msg += f"âš¡ å‡¦ç†é–“éš”: {REALTIME_CHUNK_DURATION_MS}ms\n"
    status_msg += f"ğŸ¯ VADæ„Ÿåº¦: {VAD_AGGRESSIVENESS}/3\n"
    status_msg += f"ğŸ”Š ç„¡éŸ³é–¾å€¤: {SILENCE_THRESHOLD_MS}ms"
    
    await ctx.send(status_msg)


@bot.event
async def on_ready():
    """Botèµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    print(f'{bot.user} ãŒDiscordã«æ¥ç¶šã—ã¾ã—ãŸï¼')
    
    global WHISPER_MODEL, transcription_engine, voice_processor
    try:
        print(f"Whisperãƒ¢ãƒ‡ãƒ« ({WHISPER_MODEL_SIZE}) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        WHISPER_MODEL = WhisperModel(WHISPER_MODEL_SIZE, device=WHISPER_DEVICE, compute_type=WHISPER_COMPUTE_TYPE)
        print("âœ… Whisperãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        
        # ã‚¨ãƒ³ã‚¸ãƒ³ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’åˆæœŸåŒ–
        transcription_engine = RealtimeTranscriptionEngine(WHISPER_MODEL, AUDIO_OUTPUT_DIR)
        voice_processor = RealtimeVoiceProcessor(transcription_engine)
        
        print("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ Whisperãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


# Botã®å®Ÿè¡Œ
if DISCORD_BOT_TOKEN:
    bot.run(DISCORD_BOT_TOKEN)
else:
    print("ã‚¨ãƒ©ãƒ¼: Discord Botãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
